#确定锻炼目标

粗略目标
----
找到python工作

目标分析
---

1. 找到python 工作 <=> 符合某个用人单位关于python的技能熟练度需求
2. 找到 需求清单
3. 搜索招聘网站
4. 筛选python分类的需求

初步目标
---------
1. 找到若干招聘网站网址列表
2. 获取python相关职位页面
3. 获取页面职位需求
4. 整理归档

初步计划
-----------
1. 先实现特定招聘网站的搜索,整理,归档
2. 再推广到多个网站(我时间有限,估计只能一个网站了)

初步选定
-----------
* 网站 拉勾网(原因,页面简单漂亮)
* 抓取方式 先尝试暴力抓取 

问题记录
-------
###抓取
1. [拉钩python结果](http://www.lagou.com/jobs/list_python?kd=python&spc=1&pl=&gj=&xl=&yx=&gx=&st=&labelWords=&lc=&workAddress=&city=%E4%B8%8A%E6%B5%B7&requestId=&pn=9)
显示为9页, 
2. 每页 document.querySelector('.hot_pos').children.length = 15
3. 如果最后一页不满15, 那总结果应该是 (15 * 8, 15 * 9]

9页的话, 我觉得还是直接拉下来比较方便

发现简单的下载下来这么一点, 我都写的很烂. 对自己无语了.

1. 保存文件名汉字乱码了.
2. 抓取文件的保存很乱,文件夹安排不合理, 应该按照关键字,城市归档的

算了, 跳过继续下一步
###分析
页面结果已经确定了其实, 现在就是把内容拉出来.

碰到问题, 感觉 只需要获得 网址, 而 拉钩的网址这么奇特
全都是 http://.*source=search 就能匹配

算是抓完了... 

但是真是越写越失望. 
生成的文件是 json 格式的. 但是第一次没有用 缩进, 美化等功能, 就1行. 然后, 美化了,打开, 发现我的文件名并不是独一无二的,而且打开内容是一串网址, 不加说明谁也不知道是干什么的.

想起来自文档这个概念, 我觉得,有时候 json应该用 {} 无论如何不用数组. {}可以添加个属性当注释, []的话, 就麻烦了. 哪怕包起来呢,呵呵.

算了,以后再补吧.差点忘了正事儿.

结果符合预期 135 个.

详情页

不用笨办法了, 上工具, scrapy

1. [参考网址](http://blog.csdn.net/pleasecallmewhy/article/details/19642329)
2. [官方网站](http://scrapy.org/)
3. [官方tutorial](http://doc.scrapy.org/en/0.24/intro/tutorial.html)

不过貌似都是从头抓取的, 没找到抓给定列表的. 直接给到初始列表里, 然后不让跟踪应该就可以了.

问题好多

1. 找不到 win32api
2. 路径不好指定
3. [pywin32](http://sourceforge.net/projects/pywin32/files/pywin32/Build%20219/pywin32-219.win32-py2.7.exe/download)

完全不知道这个有什么用. 不过安装之后下载成功了. 但是没有保存

处理部分位于 parse 函数.. 现在是pass,所以 ...orz

看到1的写法是, 直接进行处理,print
然后把所有输出重定向到文件里.

这个parse 应该是每一个 respone 的回调函数.那找个文件,来存下来估计最好.

response 能调用 re, xpath, 也算是不错了.
我试试看


好吧, 看似所有的内容 说是css查询,其实还是xpath. 所以, xpath查询里可以添加 text() 但是
css就不行了. 所以,最好是直接自己搞定, css查询结束,再跟上.xpath转化一次. 

现在问题是, 返回的都是列表,可能需要那啥 strip一下,join一下, 去掉所有的空格.

另外这网站虽然很漂亮, 但并不好提取. 右侧居然有大量没用的注释.而且部分不分class,直接是循环输出的, 只能数. 但是scrapy确实好强大


初步感觉我一个一个匹配 item, 用css来查是不对的. 其实应该把css或者 xpath表达式放到item里, 这样可以有效的循环.


漏抓了  网站和公司真名,还有一个投资阶段.

公司真名是在传说中的 图标的 alt里, 奇葩.

碰到一个问题, 抓到的是 div 然后, 不知道 xpath里 累死 innerText那种函数有没有. text函数似乎没有返回子元素里的内容.

实在不行, 只好用 * 来匹配.

现在在考虑抓了怎么办了, 简单的就是输出个csv,然后excel比较下. 再或者, 不行就是直接 写个web,集合所有结果做个列表. 但这样和原始网站就没区别了啊,我还抓他干嘛.

算了, 还有正事儿. 先做个列表吧,方便.

#列表显示
单页足够,所以, webpy估计就行了. 
又迷茫了. 如果开放 file路径的话, 这里不用服务器也可以的样子.

列表倒是很简单, 一下就出来了. 但是原始的需求部分是 一堆p, 保留标签的话, 模版会过滤掉, 不保留的话,换行还是很乱.render时候能把\n换成br之类就好了. 或者这部分数据我直接存成list


排序输出 
看到一个问题, 直接漏抓了[一个公司](
http://www.lagou.com/jobs/202725.html?source=search) 他们换行用的不是p, 所以, 我全抓错了.
o(︶︿︶)o 唉

总算回来了. 继续.
发现这里的lagou基本都是高大上职位啊.而且很多要求都很不好说, 能完全合适都没有.有的要求只能满足一部分...

看到很多专门招爬虫的.这部分,需要很多东西的要求, 我似乎是弱了.regex,xpath,等等还好,但是多线程并发抓取. 这什么景,真没考虑过.应该是因为数量的差异,导致这些大量内容时候,必须用高级方法. 路很长啊


昨天面了个试...其实挺绝望的今天.

python 为名的职位,大概是
51job 140, lagou135, 智联 80? 

总体来说, 其实没啥需求.不过在正文里有python的公司, 51 和智联都是2k多个.
全抓下不难,分析起来就没意思了. 甚至有几个搅浑水的, 需求和公司简介完全就是瞎写. 

最有意思的就是丫丫这个网站名, 被人给抄走了, 出现了好多次.
